{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ['DISPLAY'] = \":11.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pcd = o3d.io.read_point_cloud(\"pcd_obj.ply\")\n",
    "pcd = o3d.io.read_point_cloud(\"pcd_scene.ply\")\n",
    "print(pcd)\n",
    "# 法线估计\n",
    "radius = 0.01  # 搜索半径\n",
    "max_nn = 30  # 邻域内用于估算法线的最大点数\n",
    "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius, max_nn))\n",
    "o3d.visualization.draw_geometries([pcd], window_name=\"法线估计\",\n",
    "                                  point_show_normal=True,\n",
    "                                  width=800,  # 窗口宽度\n",
    "                                  height=600)  # 窗口高度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.dom.minidom\n",
    "import yaml\n",
    "\n",
    "def RemoveXml_space(xml_path):\n",
    "    with open(xml_path,\"r\") as f:\n",
    "        res = f.readlines() #res 为列表\n",
    "    res = [x for x in res if x.split()] #将空行从 res 中去掉\n",
    "    \n",
    "    with open(xml_path,\"w\") as f:\n",
    "        f.write(\"\".join(res))  #将 res 转换为 字符串重写写入到xml\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"experiment/resources/objects/pybullet_ycb\"\n",
    "# with open(f'/home/shixin/data/pybullet_ycb/config.yml','r') as ff:\n",
    "#     cfg = yaml.safe_load(ff)\n",
    "\n",
    "for i, file in  enumerate(sorted(os.listdir(input_dir))):\n",
    "    obj_dir = os.path.join(input_dir, file)\n",
    "    # ms.load_new_mesh(os.path.join(obj_dir, \"textured.obj\"))\n",
    "\n",
    "    # # compute the geometric measures of the current mesh\n",
    "    # # and save the results in the out_dict dictionary\n",
    "    # out_dict = ms.get_geometric_measures()\n",
    "\n",
    "    # (3, )\n",
    "    # com = out_dict[\"center_of_mass\"]\n",
    "    # # (3, 3)\n",
    "    # iner_matrix = out_dict[\"inertia_tensor\"]\n",
    "    # inertia_xx = iner_matrix[0, 0]\n",
    "    # inertia_xy = iner_matrix[0, 1]\n",
    "    # inertia_xz = iner_matrix[0, 2]\n",
    "    # inertia_yy = iner_matrix[1, 1]\n",
    "    # inertia_yz = iner_matrix[1, 2]\n",
    "    # inertia_zz = iner_matrix[2, 2]\n",
    "\n",
    "    # print(com)\n",
    "\n",
    "    # mass = cfg['mass'][i]\n",
    "\n",
    "    # if os.path.exists(os.path.join(obj_dir, \"model.xml\")):\n",
    "    #     os.remove(os.path.join(obj_dir, \"model.xml\"))\n",
    "    \n",
    "    # if os.path.exists(os.path.join(obj_dir, \"model_tune.urdf\")):\n",
    "    #     os.replace(os.path.join(obj_dir, \"model_tune.urdf\"), os.path.join(obj_dir, \"model.urdf\"))\n",
    "\n",
    "    # if os.path.exists(os.path.join(obj_dir, \"model.urdf\")):\n",
    "    #     os.replace(os.path.join(obj_dir, \"model.urdf\"), os.path.join(obj_dir, \"model.xml\"))\n",
    "\n",
    "    RemoveXml_space(os.path.join(obj_dir, \"model.xml\"))\n",
    "\n",
    "    if os.path.exists(os.path.join(obj_dir, \"model.xml\")):\n",
    "        os.replace(os.path.join(obj_dir, \"model.xml\"), os.path.join(obj_dir, \"model.urdf\"))\n",
    "\n",
    "    \n",
    "    # os.remove(os.path.join(obj_dir, \"model_tune.urdf\"))\n",
    "    # os.replace(os.path.join(obj_dir, \"model_tune.urdf\"), os.path.join(obj_dir, \"model_tune.xml\"))\n",
    "    # xmlDoc = xml.dom.minidom.parse(os.path.join(obj_dir, \"model.urdf\"))\n",
    "    # xmlDoc.getElementsByTagName(\"mass\")[0].setAttribute(\"value\", str(mass * 0.001))\n",
    "    \n",
    "    # xmlDoc.getElementsByTagName(\"inertia\")[0].setAttribute(\"ixx\", str(1e-3))\n",
    "    # xmlDoc.getElementsByTagName(\"inertia\")[0].setAttribute(\"ixy\", str(0))\n",
    "    # xmlDoc.getElementsByTagName(\"inertia\")[0].setAttribute(\"ixz\", str(0))\n",
    "    # xmlDoc.getElementsByTagName(\"inertia\")[0].setAttribute(\"iyy\", str(1e-3))\n",
    "    # xmlDoc.getElementsByTagName(\"inertia\")[0].setAttribute(\"iyz\", str(0))\n",
    "    # xmlDoc.getElementsByTagName(\"inertia\")[0].setAttribute(\"izz\", str(1e-3))\n",
    "    # com_value = xmlDoc.getElementsByTagName(\"origin\")[0].getAttribute(\"xyz\")\n",
    "    # print(f\"before com value: {com_value}\")\n",
    "    # xmlDoc.getElementsByTagName(\"origin\")[0].setAttribute(\"xyz\", f\"{com[0]} {com[1]} {com[2]}\")\n",
    "    # com_value = xmlDoc.getElementsByTagName(\"origin\")[0].getAttribute(\"xyz\")\n",
    "    # print(f\"after com value: {com_value}\")\n",
    "    # os.replace(os.path.join(obj_dir, \"model_tune.xml\"), os.path.join(obj_dir, \"model_tune.urdf\"))\n",
    "    # with open(os.path.join(obj_dir, \"model.urdf\"), 'wb') as f:\n",
    "    #     f.write(xmlDoc.toprettyxml(encoding='utf-8'))\n",
    "    # os.remove(os.path.join(obj_dir, \"model_tune.xml\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "RemoveXml_space(\"experiment/resources/objects/pybullet_ycb/035_power_drill/model.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = {'小萌': '1001', '小智': '1002', '小强': '1003', '小明': '1004'}\n",
    "try:\n",
    "    list (student.keys()) [list (student.values()).index ('100')]\n",
    "except:\n",
    "    print('没有找到对应的值')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspnetAPI.graspnet import GraspNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graspnet_train = GraspNet(root=\"data/Benchmark/graspnet\", camera=\"realsense\", split=\"train\")\n",
    "print(f\"训练物体: \\n {graspnet_train.objIds}\")\n",
    "\n",
    "graspnet_train = GraspNet(root=\"data/Benchmark/graspnet\", camera=\"realsense\", split=\"test_seen\")\n",
    "print(f\"seen 物体: \\n {graspnet_train.objIds}\")\n",
    "\n",
    "graspnet_train = GraspNet(root=\"data/Benchmark/graspnet\", camera=\"realsense\", split=\"test_similar\")\n",
    "print(f\"similar: \\n {graspnet_train.objIds}\")\n",
    "\n",
    "graspnet_train = GraspNet(root=\"data/Benchmark/graspnet\", camera=\"realsense\", split=\"test_novel\")\n",
    "print(f\"novel: \\n {graspnet_train.objIds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(8):\n",
    "    num_objs = random.sample(range(5, 8), 1)\n",
    "    random_numbers = random.sample(range(11), num_objs[0])\n",
    "    print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例数组，包含 True 和 False 值\n",
    "array = np.array([True, False, True, True, True, False, True, False])\n",
    "\n",
    "# 使用 count() 函数统计值为 True 的元素个数\n",
    "count_true = np.count_nonzero(array)\n",
    "\n",
    "# 输出值为 True 的元素个数\n",
    "print(\"值为 True 的元素个数为:\", count_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化指定场景视角的关键点和抓取位姿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to import geometry msgs in rigid_transformations.py.\n",
      "WARNING:root:Failed to import ros dependencies in rigid_transforms.py\n",
      "WARNING:root:autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as scio\n",
    "from models.graspnet import GraspNet, pred_decode\n",
    "from dataset.graspnet_dataset import GraspNetDataset, collate_fn\n",
    "from graspnetAPI import GraspGroup, GraspNetEval\n",
    "from graspnetAPI.utils.utils import generate_scene_model\n",
    "from graspnetAPI.utils.eval_utils import get_scene_name, create_table_points, parse_posevector, \\\n",
    "    load_dexnet_model, transform_points, compute_point_distance, compute_closest_points, \\\n",
    "        voxel_sample_points, topk_grasps, get_grasp_score, collision_detection, eval_grasp\n",
    "from utils.data_utils import CameraInfo, transform_point_cloud, \\\n",
    "    create_point_cloud_from_depth_image, get_workspace_mask, remove_invisible_grasp_points\n",
    "from utils.collision_detector import ModelFreeCollisionDetector\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "from experiment.utils import toOpen3dCloud\n",
    "os.environ['DISPLAY'] = \":11.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data path and collision labels...:   0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data path and collision labels...: 100%|██████████| 90/90 [00:00<00:00, 456.18it/s]\n",
      "Loading data path...: 100%|██████████| 90/90 [00:00<00:00, 351.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET = GraspNetDataset(\"data/Benchmark/graspnet\", valid_obj_idxs=None, grasp_labels=None, split=\"test\", \n",
    "                               camera=\"realsense\", num_points=20000, remove_outlier=True, augment=False, load_label=False)\n",
    "\n",
    "ge = GraspNetEval(root=\"data/Benchmark/graspnet\", camera=\"realsense\", split=\"test\")\n",
    "net = GraspNet(input_feature_dim=0, num_view=300, num_angle=12, num_depth=4, \n",
    "               cylinder_radius=0.05, hmin=-0.02, hmax_list=[0.01,0.02,0.03,0.04], is_training=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"logs/log_rs/2023-09-10-09-44/checkpoint.tar\")\n",
    "net.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(scene_id, img_id, raw=False):\n",
    "    ret_dict = TEST_DATASET.get_data((scene_id - 100) * 256 + img_id, return_raw_cloud=raw)\n",
    "    # if raw == False:\n",
    "    #     pcd = ret_dict['point_clouds']\n",
    "    #     obj_mask = ret_dict['seg_mask']\n",
    "    #     pcd_obj_inds = np.argwhere(obj_mask>0).squeeze() # (N_obj,)\n",
    "    #     pcd_obj = pcd[obj_mask>0]\n",
    "    #     num_pts_obj = 1024\n",
    "    #     if len(pcd_obj) >= num_pts_obj:\n",
    "    #         idxs = np.random.choice(len(pcd_obj), num_pts_obj, replace=False)\n",
    "    #     else:\n",
    "    #         idxs1 = np.arange(len(pcd_obj))\n",
    "    #         idxs2 = np.random.choice(len(pcd_obj), num_pts_obj-len(pcd_obj), replace=True)\n",
    "    #         idxs = np.concatenate([idxs1, idxs2], axis=0)\n",
    "    #     pcd_obj_inds = pcd_obj_inds[idxs] # (1024, )\n",
    "    #     ret_dict['pcd_obj_inds'] = pcd_obj_inds\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene_id = 100\n",
    "img_id = 0\n",
    "\n",
    "for scene_id in range(103, 106):\n",
    "    # 点云，颜色，segmentation， 物体点索引\n",
    "    data = [get_data(scene_id, img_id)]\n",
    "\n",
    "    # 扩展 batch 维度\n",
    "    batch_data = collate_fn(data)\n",
    "\n",
    "    # 送到 GPU\n",
    "    for key in batch_data:\n",
    "        if 'list' in key:\n",
    "            for i in range(len(batch_data[key])):\n",
    "                for j in range(len(batch_data[key][i])):\n",
    "                    batch_data[key][i][j] = batch_data[key][i][j].to(device)\n",
    "        else:\n",
    "            batch_data[key] = batch_data[key].to(device)\n",
    "\n",
    "    # Forward pass，保存关键点和场景\n",
    "    with torch.no_grad():\n",
    "        end_points = net(batch_data)\n",
    "        grasp_preds = pred_decode(end_points)\n",
    "\n",
    "    vis_result_dir = f\"vis_result/{scene_id}_{img_id}\"\n",
    "    if not os.path.exists(vis_result_dir):\n",
    "        os.makedirs(vis_result_dir)\n",
    "\n",
    "    # 抓取位姿送到 CPU\n",
    "    preds = grasp_preds[0].detach().cpu().numpy()\n",
    "    gg = GraspGroup(preds)\n",
    "    num_pred = len(gg)\n",
    "    # total_pred += num_pred\n",
    "\n",
    "    # 抓取点 (1024, 3)\n",
    "    graspable_p = preds[:, 13:16]\n",
    "    graspable_pcd_o3d = toOpen3dCloud(graspable_p)\n",
    "    o3d.io.write_point_cloud(f\"{vis_result_dir}/graspable_pcd.ply\", graspable_pcd_o3d)\n",
    "\n",
    "    # 旋转矩阵\n",
    "    # rot_matrix = preds[:, 4:13]\n",
    "    # rot_matrix = rot_matrix.reshape((1024, 3, 3))\n",
    "    # (1024, 3)\n",
    "    # approach_vector = rot_matrix[:, :, 0]\n",
    "    # approach_vector_o3d = toOpen3dCloud(approach_vector)\n",
    "    # o3d.io.write_point_cloud(f\"{vis_result_dir}/approach_vector.ply\", approach_vector_o3d)\n",
    "\n",
    "    # 原始规模的点云\n",
    "    cloud, rgb = get_data(scene_id, img_id, raw=True)\n",
    "    # 保存到本地\n",
    "    raw_pcd_o3d = toOpen3dCloud(cloud, rgb)\n",
    "    o3d.io.write_point_cloud(f\"{vis_result_dir}/raw_pcd.ply\", raw_pcd_o3d)\n",
    "\n",
    "    # collision detection \n",
    "    # mfcdetector = ModelFreeCollisionDetector(cloud, voxel_size=0.01)\n",
    "    # collision_mask = mfcdetector.detect(gg, approach_dist=0.05, collision_thresh=0.01)\n",
    "    # num_coll = np.count_nonzero(collision_mask)\n",
    "    # # total_coll += num_coll\n",
    "    # print(f\"collision rate: {(num_coll/num_pred):.2f}\\n\")\n",
    "    # gg = gg[~collision_mask]\n",
    "\n",
    "    # 可视化抓取\n",
    "    # table = create_table_points(1.0, 1.0, 0.05, dx=-0.5, dy=-0.5, dz=-0.05, grid_size=0.008)\n",
    "    # _, pose_list, camera_pose, align_mat = ge.get_model_poses(scene_id, img_id)\n",
    "    # table_trans = transform_points(table, np.linalg.inv(np.matmul(align_mat, camera_pose)))\n",
    "    # t = o3d.geometry.PointCloud()\n",
    "    # t.points = o3d.utility.Vector3dVector(table_trans)\n",
    "    # model_list = generate_scene_model(\"data/Benchmark/graspnet\", 'scene_%04d' % scene_id , \n",
    "                                    #   img_id, return_poses=False, align=False, camera=\"realsense\")\n",
    "    nms_gg = gg.nms()\n",
    "    nms_gg = nms_gg[:5]\n",
    "    grasps_geometry = nms_gg.to_open3d_geometry_list()\n",
    "    \n",
    "    pcd = ge.loadScenePointCloud(scene_id, \"realsense\", img_id)\n",
    "    o3d.visualization.draw_geometries([pcd, *grasps_geometry])\n",
    "    # o3d.visualization.draw_geometries([pcd, *grasps_geometry, *model_list])\n",
    "    # o3d.visualization.draw_geometries([*grasps_geometry, *model_list, t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## robotiq-85 gripper 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FreeCAD as App\n",
    "import ImportGui\n",
    "import Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Step文件路径\n",
    "step_file = \"/path/to/your/step/file.step\"\n",
    "# 定义STL文件路径\n",
    "stl_file = \"/path/to/your/stl/file.stl\"\n",
    "\n",
    "# 初始化FreeCAD应用\n",
    "App.newDocument(\"Unnamed\")\n",
    "\n",
    "# 导入Step文件\n",
    "ImportGui.open(step_file)\n",
    "\n",
    "# 获取导入的Step对象\n",
    "shape = App.ActiveDocument.ActiveObject.Shape\n",
    "\n",
    "# 将Step对象转换为Mesh对象\n",
    "mesh = Mesh.Mesh()\n",
    "mesh.fromShape(shape)\n",
    "\n",
    "# 将Mesh对象保存为STL文件\n",
    "mesh.write(stl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onebillion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
