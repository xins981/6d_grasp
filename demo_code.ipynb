{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化指定场景视角的关键点和抓取位姿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n",
      "WARNING:root:Failed to import geometry msgs in rigid_transformations.py.\n",
      "WARNING:root:Failed to import ros dependencies in rigid_transforms.py\n",
      "WARNING:root:autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as scio\n",
    "from models.graspnet import GraspNet, pred_decode\n",
    "from dataset.graspnet_dataset import GraspNetDataset, collate_fn\n",
    "from graspnetAPI import GraspGroup, GraspNetEval\n",
    "from graspnetAPI.utils.utils import generate_scene_model\n",
    "from graspnetAPI.utils.eval_utils import get_scene_name, create_table_points, parse_posevector, \\\n",
    "    load_dexnet_model, transform_points, compute_point_distance, compute_closest_points, \\\n",
    "        voxel_sample_points, topk_grasps, get_grasp_score, collision_detection, eval_grasp\n",
    "from utils.data_utils import CameraInfo, transform_point_cloud, \\\n",
    "    create_point_cloud_from_depth_image, get_workspace_mask, remove_invisible_grasp_points\n",
    "from utils.collision_detector import ModelFreeCollisionDetector\n",
    "import numpy as np\n",
    "from experiment.utils import toOpen3dCloud\n",
    "import open3d as o3d\n",
    "import os\n",
    "os.environ['DISPLAY'] = \":11.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data path and collision labels...: 100%|██████████| 90/90 [00:00<00:00, 321.02it/s]\n",
      "Loading data path...: 100%|██████████| 90/90 [00:00<00:00, 238.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> loaded checkpoint logs/log_rs_spotr/2023-10-06-23-21/checkpoint.tar (epoch: 18)\n"
     ]
    }
   ],
   "source": [
    "TEST_DATASET = GraspNetDataset(\"data/Benchmark/graspnet\", valid_obj_idxs=None, grasp_labels=None, split=\"test\", \n",
    "                               camera=\"realsense\", num_points=20000, remove_outlier=True, \n",
    "                               augment=False, load_label=False)\n",
    "\n",
    "ge = GraspNetEval(root=\"data/Benchmark/graspnet\", camera=\"realsense\", split=\"test\")\n",
    "net = GraspNet(input_feature_dim=0, num_view=300, num_angle=12, num_depth=4, \n",
    "               cylinder_radius=0.05, hmin=-0.02, hmax_list=[0.01,0.02,0.03,0.04], is_training=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"logs/log_rs_spotr/2023-10-06-23-21/checkpoint.tar\"\n",
    "# checkpoint_path = \"logs/log_rs_spotr/2023-11-07-17-30/checkpoint.tar\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "print(f\"-> loaded checkpoint {checkpoint_path} (epoch: {start_epoch})\")\n",
    "# net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(scene_id, img_id, raw=False):\n",
    "    ret_dict = TEST_DATASET.get_data((scene_id - 100) * 256 + img_id, return_raw_cloud=raw)\n",
    "    if raw == False:\n",
    "        pcd = ret_dict['point_clouds']\n",
    "        obj_mask = ret_dict['seg_mask']\n",
    "        pcd_obj_inds = np.argwhere(obj_mask>0).squeeze() # (N_obj,)\n",
    "        pcd_obj = pcd[obj_mask>0]\n",
    "        num_pts_obj = 1024\n",
    "        if len(pcd_obj) >= num_pts_obj:\n",
    "            idxs = np.random.choice(len(pcd_obj), num_pts_obj, replace=False)\n",
    "        else:\n",
    "            idxs1 = np.arange(len(pcd_obj))\n",
    "            idxs2 = np.random.choice(len(pcd_obj), num_pts_obj-len(pcd_obj), replace=True)\n",
    "            idxs = np.concatenate([idxs1, idxs2], axis=0)\n",
    "        pcd_obj_inds = pcd_obj_inds[idxs] # (1024, )\n",
    "        ret_dict['pcd_obj_inds'] = pcd_obj_inds\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision rate: 0.70\n",
      "\n",
      "collision rate: 0.72\n",
      "\n",
      "collision rate: 0.62\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shixin/code/6d_grasp/demo_code.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090/home/shixin/code/6d_grasp/demo_code.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m grasps_geometry \u001b[39m=\u001b[39m nms_gg\u001b[39m.\u001b[39mto_open3d_geometry_list()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090/home/shixin/code/6d_grasp/demo_code.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m pcd \u001b[39m=\u001b[39m ge\u001b[39m.\u001b[39mloadScenePointCloud(scene_id, \u001b[39m\"\u001b[39m\u001b[39mrealsense\u001b[39m\u001b[39m\"\u001b[39m, img_id)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B4090/home/shixin/code/6d_grasp/demo_code.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m o3d\u001b[39m.\u001b[39;49mvisualization\u001b[39m.\u001b[39;49mdraw_geometries([pcd, \u001b[39m*\u001b[39;49mgrasps_geometry])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090/home/shixin/code/6d_grasp/demo_code.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# o3d.visualization.draw_geometries([pcd, *grasps_geometry, *model_list])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090/home/shixin/code/6d_grasp/demo_code.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# o3d.visualization.draw_geometries([*grasps_geometry, *model_list, t])\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# scene_id = 100\n",
    "img_id = 0\n",
    "\n",
    "for scene_id in range(103, 106):\n",
    "    # 点云，颜色，segmentation， 物体点索引\n",
    "    data = [get_data(scene_id, img_id)]\n",
    "\n",
    "    # 扩展 batch 维度\n",
    "    batch_data = collate_fn(data)\n",
    "\n",
    "    # 送到 GPU\n",
    "    for key in batch_data:\n",
    "        if 'list' in key:\n",
    "            for i in range(len(batch_data[key])):\n",
    "                for j in range(len(batch_data[key][i])):\n",
    "                    batch_data[key][i][j] = batch_data[key][i][j].to(device)\n",
    "        else:\n",
    "            batch_data[key] = batch_data[key].to(device)\n",
    "\n",
    "    # Forward pass，保存关键点和场景\n",
    "    with torch.no_grad():\n",
    "        end_points = net(batch_data)\n",
    "        grasp_preds = pred_decode(end_points)\n",
    "\n",
    "    vis_result_dir = f\"vis_result/{scene_id}_{img_id}\"\n",
    "    if not os.path.exists(vis_result_dir):\n",
    "        os.makedirs(vis_result_dir)\n",
    "\n",
    "    # 保存关键点\n",
    "    global_p = end_points['global_p']\n",
    "    global_p = global_p[4][0].detach().cpu().numpy()\n",
    "    global_p_o3d = toOpen3dCloud(global_p)\n",
    "    o3d.io.write_point_cloud(f\"{vis_result_dir}/keys.ply\", global_p_o3d)\n",
    "\n",
    "    # 抓取位姿送到 CPU\n",
    "    preds = grasp_preds[0].detach().cpu().numpy()\n",
    "    gg = GraspGroup(preds)\n",
    "    num_pred = len(gg)\n",
    "    # total_pred += num_pred\n",
    "\n",
    "    # 抓取点 (1024, 3)\n",
    "    graspable_p = preds[:, 13:16]\n",
    "    graspable_pcd_o3d = toOpen3dCloud(graspable_p)\n",
    "    o3d.io.write_point_cloud(f\"{vis_result_dir}/graspness.ply\", graspable_pcd_o3d)\n",
    "\n",
    "    # 原始规模的点云\n",
    "    cloud, rgb = get_data(scene_id, img_id, raw=True)\n",
    "    # 保存到本地\n",
    "    raw_pcd_o3d = toOpen3dCloud(cloud, rgb)\n",
    "    o3d.io.write_point_cloud(f\"{vis_result_dir}/raw_pcd.ply\", raw_pcd_o3d)\n",
    "     \n",
    "    # collision detection \n",
    "    cloud, _ = get_data(scene_id, img_id, raw=True)\n",
    "    mfcdetector = ModelFreeCollisionDetector(cloud, voxel_size=0.01)\n",
    "    collision_mask = mfcdetector.detect(gg, approach_dist=0.05, collision_thresh=0.01)\n",
    "    num_coll = np.count_nonzero(collision_mask)\n",
    "    # total_coll += num_coll\n",
    "    print(f\"collision rate: {(num_coll/num_pred):.2f}\\n\")\n",
    "    gg = gg[~collision_mask]\n",
    "\n",
    "    # 可视化抓取\n",
    "    # table = create_table_points(1.0, 1.0, 0.05, dx=-0.5, dy=-0.5, dz=-0.05, grid_size=0.008)\n",
    "    # _, pose_list, camera_pose, align_mat = ge.get_model_poses(scene_id, img_id)\n",
    "    # table_trans = transform_points(table, np.linalg.inv(np.matmul(align_mat, camera_pose)))\n",
    "    # t = o3d.geometry.PointCloud()\n",
    "    # t.points = o3d.utility.Vector3dVector(table_trans)\n",
    "    # model_list = generate_scene_model(\"data/Benchmark/graspnet\", 'scene_%04d' % scene_id , \n",
    "    #                                 img_id, return_poses=False, align=False, camera=\"realsense\")\n",
    "\n",
    "    nms_gg = gg.nms()\n",
    "    nms_gg = nms_gg[:10]\n",
    "    grasps_geometry = nms_gg.to_open3d_geometry_list()\n",
    "    pcd = ge.loadScenePointCloud(scene_id, \"realsense\", img_id)\n",
    "    o3d.visualization.draw_geometries([pcd, *grasps_geometry])\n",
    "    # o3d.visualization.draw_geometries([pcd, *grasps_geometry, *model_list])\n",
    "    # o3d.visualization.draw_geometries([*grasps_geometry, *model_list, t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log 抓取分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 0.1 = 2.3026\n",
      "log 0.2 = 1.6094\n",
      "log 0.30000000000000004 = 1.2040\n",
      "log 0.4 = 0.9163\n",
      "log 0.5 = 0.6931\n",
      "log 0.6 = 0.5108\n",
      "log 0.7000000000000001 = 0.3567\n",
      "log 0.8 = 0.2231\n",
      "log 0.9 = 0.1054\n",
      "log 1.0 = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1, 1.1, 0.1):\n",
    "    print(f\"log {i} = {np.log(1/i):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传播 top-k 交叉熵 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4968)\n"
     ]
    }
   ],
   "source": [
    "# 假设你有一个模型的预测值 predictions 和对应的目标真值 targets\n",
    "predictions = torch.randn((2, 2, 1024))  # 模型的预测值\n",
    "targets = torch.randint(0, 1, (2, 1024)).long()  # 随机生成一个目标标签，这里假设有1000个类别\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# 计算误差（可以是任何形式的误差，如交叉熵、均方误差等）\n",
    "errors = criterion(predictions, targets)\n",
    "\n",
    "# 使用torch.topk获取前512个最大误差项的索引\n",
    "topk_values, topk_indices = torch.topk(errors, k=512)\n",
    "\n",
    "# 仅保留前512个最大误差项的损失\n",
    "loss = torch.mean(topk_values)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 点云渲染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center: [-0.02210835  0.03199192  0.4575    ], Scale: 0.3523904354109818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48287/1351205205.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pcl = np.zeros(data_pd.shape, dtype=np.float)  # 初始化储存数据的array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from plyfile import PlyData, PlyElement\n",
    "import pandas as pd\n",
    " \n",
    "file_dir = 'vis_result/103_0/raw_pcd.ply'  #文件的路径\n",
    "plydata = PlyData.read(file_dir)  # 读取文件\n",
    "data = plydata.elements[0].data  # 读取数据\n",
    "data_pd = pd.DataFrame(data)  # 转换成DataFrame, 因为DataFrame可以解析结构化的数据\n",
    "pcl = np.zeros(data_pd.shape, dtype=np.float)  # 初始化储存数据的array\n",
    "property_names = data[0].dtype.names  # 读取property的名字\n",
    "for i, name in enumerate(property_names):  # 按property读取数据，这样可以保证读出的数据是同样的数据类型。\n",
    "    pcl[:, i] = data_pd[name]\n",
    "\n",
    "def standardize_bbox(pcl, points_per_object):\n",
    "    pt_indices = np.random.choice(pcl.shape[0], points_per_object, replace=False)\n",
    "    np.random.shuffle(pt_indices)\n",
    "    pcl = pcl[pt_indices] # n by 3\n",
    "    mins = np.amin(pcl, axis=0)\n",
    "    maxs = np.amax(pcl, axis=0)\n",
    "    center = ( mins + maxs ) / 2.\n",
    "    scale = np.amax(maxs-mins)\n",
    "    print(\"Center: {}, Scale: {}\".format(center, scale))\n",
    "    result = ((pcl - center)/scale).astype(np.float32) # [-0.5, 0.5]\n",
    "    return result\n",
    "\n",
    "xml_head = \\\n",
    "\"\"\"\n",
    "<scene version=\"0.6.0\">\n",
    "    <integrator type=\"path\">\n",
    "        <integer name=\"maxDepth\" value=\"-1\"/>\n",
    "    </integrator>\n",
    "    <sensor type=\"perspective\">\n",
    "        <float name=\"farClip\" value=\"100\"/>\n",
    "        <float name=\"nearClip\" value=\"0.1\"/>\n",
    "        <transform name=\"toWorld\">\n",
    "            <lookat origin=\"3,3,3\" target=\"0,0,0\" up=\"0,0,1\"/>\n",
    "        </transform>\n",
    "        <float name=\"fov\" value=\"25\"/>\n",
    "        \n",
    "        <sampler type=\"ldsampler\">\n",
    "            <integer name=\"sampleCount\" value=\"256\"/>\n",
    "        </sampler>\n",
    "        <film type=\"hdrfilm\">\n",
    "            <integer name=\"width\" value=\"1600\"/>\n",
    "            <integer name=\"height\" value=\"1200\"/>\n",
    "            <rfilter type=\"gaussian\"/>\n",
    "            <boolean name=\"banner\" value=\"false\"/>\n",
    "        </film>\n",
    "    </sensor>\n",
    "    \n",
    "    <bsdf type=\"roughplastic\" id=\"surfaceMaterial\">\n",
    "        <string name=\"distribution\" value=\"ggx\"/>\n",
    "        <float name=\"alpha\" value=\"0.05\"/>\n",
    "        <float name=\"intIOR\" value=\"1.46\"/>\n",
    "        <rgb name=\"diffuseReflectance\" value=\"1,1,1\"/> <!-- default 0.5 -->\n",
    "    </bsdf>\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "xml_ball_segment = \\\n",
    "\"\"\"\n",
    "    <shape type=\"sphere\">\n",
    "        <float name=\"radius\" value=\"0.025\"/>\n",
    "        <transform name=\"toWorld\">\n",
    "            <translate x=\"{}\" y=\"{}\" z=\"{}\"/>\n",
    "        </transform>\n",
    "        <bsdf type=\"diffuse\">\n",
    "            <rgb name=\"reflectance\" value=\"{},{},{}\"/>\n",
    "        </bsdf>\n",
    "    </shape>\n",
    "\"\"\"\n",
    "\n",
    "xml_tail = \\\n",
    "\"\"\"\n",
    "    <shape type=\"rectangle\">\n",
    "        <ref name=\"bsdf\" id=\"surfaceMaterial\"/>\n",
    "        <transform name=\"toWorld\">\n",
    "            <scale x=\"10\" y=\"10\" z=\"1\"/>\n",
    "            <translate x=\"0\" y=\"0\" z=\"-0.5\"/>\n",
    "        </transform>\n",
    "    </shape>\n",
    "    \n",
    "    <shape type=\"rectangle\">\n",
    "        <transform name=\"toWorld\">\n",
    "            <scale x=\"10\" y=\"10\" z=\"1\"/>\n",
    "            <lookat origin=\"-4,4,20\" target=\"0,0,0\" up=\"0,0,1\"/>\n",
    "        </transform>\n",
    "        <emitter type=\"area\">\n",
    "            <rgb name=\"radiance\" value=\"6,6,6\"/>\n",
    "        </emitter>\n",
    "    </shape>\n",
    "</scene>\n",
    "\"\"\"\n",
    "\n",
    "def colormap(x,y,z):\n",
    "    vec = np.array([x,y,z])\n",
    "    vec = np.clip(vec, 0.001,1.0)\n",
    "    norm = np.sqrt(np.sum(vec**2))\n",
    "    vec /= norm\n",
    "    return [vec[0], vec[1], vec[2]]\n",
    "xml_segments = [xml_head]\n",
    "\n",
    "pcl = pcl[:, :3]\n",
    "# pcl = np.load('chair_pcl.npy')\n",
    "pcl = standardize_bbox(pcl, 2048)\n",
    "pcl = pcl[:,[2,0,1]] # z, x, y\n",
    "pcl[:,0] *= -1 # -z, x, y\n",
    "pcl[:,2] += 0.0125 # -z, x, y+0.0125\n",
    "\n",
    "for i in range(pcl.shape[0]):\n",
    "    color = colormap(pcl[i,0]+0.5,pcl[i,1]+0.5,pcl[i,2]+0.5-0.0125)\n",
    "    xml_segments.append(xml_ball_segment.format(pcl[i,0],pcl[i,1],pcl[i,2], *color))\n",
    "xml_segments.append(xml_tail)\n",
    "\n",
    "xml_content = str.join('', xml_segments)\n",
    "\n",
    "with open('mitsuba_scene.xml', 'w') as f:\n",
    "    f.write(xml_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onebillion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
